#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Dec 18 17:36:58 2017

@author: johnbauer
"""

from qualitative_kernels import *

# =============================================================================
# TODO: there should be no difference between putting a single RBF kernel
# with individual length scales on the set of dummy-coded variables for a factor,
# and the product of individual RBF kernels as done here.
# So why not use the simpler implementation?
#     kernel = ck.Projection(columns, name="factor",
#                            kernel=RBF([1.0]*len(columns)))
# =============================================================================
class SimpleFactorKernel(Tensor):
    """Alternative implementation of SimpleCategoricalKernel
    
    Testing the water with CompoundKernel before attempting Tensor
    """
    def __init__(self, columns):     #, length_scale, length_scale_bounds=()):
        """Dummy-code the given column, put a RBF kernel
        on each of the variates, then return the product kernel
        
        If all length scales are small, assume little shared information
        between categories
        
        kernel will typically be RBF with a single length parameter
        (passing in an alternative kernel not currently implemented)
        """
#        assert isinstance(column, (list, tuple, int)), "must be int or list of ints"
#        self.column = [column] if isinstance(column, int) else column
#        assert all(isinstance(i, int) for i in self.column), "must be integers"
        self.columns = columns        

        kernels = [Projection([c]) for c in columns]
                                    #factor_name(c)) for c in columns]
    
        # collect all the kernels to be combined into a single product kernel
        super(SimpleFactorKernel, self).__init__(kernels)    

    def get_params(self, deep=True):
        """Get parameters of this kernel.

        Parameters
        ----------
        deep: boolean, optional
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.

        Returns
        -------
        params : mapping of string to any
            Parameter names mapped to their values.
        """
        #params = dict(kernel=self.kernel, dim=self.dim)
        params = dict(columns=self.columns)
        if deep:
            for i, kernel in enumerate(self.kernels):
                print("--->", "\ti = ", i, "\tkernel = ", kernel)
                deep_items = kernel.get_params().items()
                #params.update((k, val) for k, val in deep_items)
                for k, val in deep_items:
                    print("\tkey = ", k, "\tvalue = ", val)
                params.update(('k{}__{}'.format(i, k), val) for k, val in deep_items)
        return params        
        
class SimpleCategoricalKernel(Kernel):
    def __init__(self, dim):     #, length_scale, length_scale_bounds=()):
        """Dummy-code the given column, put a RBF kernel
        on each of the variates, then return the product kernel
        
        If all length scales are small, assume little shared information
        between categories
        
        kernel will typically be RBF with a single length parameter
        (passing in an alternative kernel not currently implemented)
        """
#        assert isinstance(column, (list, tuple, int)), "must be int or list of ints"
#        self.column = [column] if isinstance(column, int) else column
#        assert all(isinstance(i, int) for i in self.column), "must be integers"
        self.dim = dim
        
        kernels = [Projection([c]) for c in range(dim)]

        # combine the kernels into a single product kernel
        self.kernel = reduce(lambda k0, k1 : k0 * k1, kernels)

    def __call__(self, X, Y=None, eval_gradient=False):
        """Assumes dummy-coded data e.g. from a single column 
        project onto each category"""

        assert X.shape[1] == self.dim, "Wrong dimension for X"
        if Y is not None:
            assert Y.shape[1] == self.dim, "Wrong dimension for Y"
        
        return self.kernel(X, Y, eval_gradient=eval_gradient)
    

#    @property
#    def hyperparameter_length_scale(self):
#        return  Hyperparameter(name="length_scale", value_type="numeric", bounds=self.bounds, n_elements=len(self.dim))
    
# =============================================================================
# Propose a UnaryOperator class to include Exponentiation kernel, 
# Projection, SimpleCategoricalKernel, and so on
# The sequel is copied wholesale from ExponentiationKernel
# =============================================================================
    def get_params(self, deep=True):
        """Get parameters of this kernel.

        Parameters
        ----------
        deep: boolean, optional
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.

        Returns
        -------
        params : mapping of string to any
            Parameter names mapped to their values.
        """
        #params = dict(kernel=self.kernel, dim=self.dim)
        params = dict(dim=self.dim)
        if deep:
            deep_items = self.kernel.get_params().items()
            params.update((k, val) for k, val in deep_items)
        return params

    @property
    def hyperparameters(self):
        """Returns a list of all hyperparameter."""
        r = []
        for hyperparameter in self.kernel.hyperparameters:
            r.append(Hyperparameter(hyperparameter.name,
                                    hyperparameter.value_type,
                                    hyperparameter.bounds,
                                    hyperparameter.n_elements,
                                    hyperparameter.log))
        return r

    @property
    def theta(self):
        """Returns the (flattened, log-transformed) non-fixed hyperparameters.

        Note that theta are typically the log-transformed values of the
        kernel's hyperparameters as this representation of the search space
        is more amenable for hyperparameter search, as hyperparameters like
        length-scales naturally live on a log-scale.

        Returns
        -------
        theta : array, shape (n_dims,)
            The non-fixed, log-transformed hyperparameters of the kernel
        """
        return self.kernel.theta

    @theta.setter
    def theta(self, theta):
        """Sets the (flattened, log-transformed) non-fixed hyperparameters.

        Parameters
        ----------
        theta : array, shape (n_dims,)
            The non-fixed, log-transformed hyperparameters of the kernel
        """
        self.kernel.theta = theta

    @property
    def bounds(self):
        """Returns the log-transformed bounds on the theta.

        Returns
        -------
        bounds : array, shape (n_dims, 2)
            The log-transformed bounds on the kernel's hyperparameters theta
        """
        return self.kernel.bounds

    def __eq__(self, b):
        if type(self) != type(b):
            return False
        return (self.kernel == b.kernel and self.dim == b.dim)


    def diag(self, X):
        """Returns the diagonal of the kernel k(X, X).

        The result of this method is identical to np.diag(self(X)); however,
        it can be evaluated more efficiently since only the diagonal is
        evaluated.

        Parameters
        ----------
        X : array, shape (n_samples_X, n_features)
            Left argument of the returned kernel k(X, Y)

        Returns
        -------
        K_diag : array, shape (n_samples_X,)
            Diagonal of kernel k(X, X)
        """
        return self.kernel.diag(X)

    def __repr__(self):
        return "{0} dummy coded on {1} dimensions".format(self.kernel, self.dim)

    def is_stationary(self):
        """Returns whether the kernel is stationary. """
        return self.kernel.is_stationary()

    
if __name__ == "__main__":
    
    X = np.array([[1,2,3,0],[2,1,3,0],[2,2,3,1],[1,4,3,2]])
    # note only two dimensions are being retained by the projection
    rbf = RBF(length_scale=np.ones(2))
    fubar = Projection([2,3], "proj", kernel=rbf)
    K = fubar(X)
    print("Projection Kernel:\n", K)
    print("Diagonal:\n", fubar.diag(X))
    
    lt = HyperSphere(2)
    print("2-parameter, lower triangular\n", lt._lower_triangular())
    
    lt1 = HyperSphere(3, [pi, pi/3, pi/6])
    print("3-parameter, lower triangular\n", lt1._lower_triangular())
    
    ltz = HyperSphere(5) #, [0.0]*10)
    print("5-parameter, lower triangular\n", ltz._lower_triangular())
    
    ltk =  UnrestrictiveCorrelation(3, [0.5,0.5,0.5])
    print("Factor Kernel\n", ltk)
    print("Factor Kernel\n", ltk(X[:,[0,1,2]]))

    #C5 = np.array([0,0,1,1,1,2,2,2,3,3]).reshape((-1,1))
    C = [0,0,1,1,1,2,2,2,3,3]
    e = [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]
    D = np.array([e[c] for c in C]).reshape(-1,4)
    #print(C5.shape)
    ltk4 =  UnrestrictiveCorrelation(4, zeta=[2*pi+pi/6]*6)
    print("Factor Kernel {} dimensions".format(ltk4.dim))
    print(ltk4)
    print(ltk4.theta)
    print(ltk4(D))
    print(ltk4.hyperparameters)
    K, G = ltk4(D, eval_gradient=True)
    print(K)
    #print(G)

# =============================================================================
#     sk = SimpleCategoricalKernel(4)
#     print(sk.get_params(deep=False))
#     print(sk.hyperparameters)
# =============================================================================
    
    prod = ltk4 * ltk4
    print(prod.get_params(deep=False))
    print(prod.hyperparameters)
    
    ltk4.theta = [pi/i for i in range(1,7)]
    print(ltk4.theta)
    print(ltk4.zeta)
    
    t = Tensor([fubar, ltk4, RBF()])
    print(t)