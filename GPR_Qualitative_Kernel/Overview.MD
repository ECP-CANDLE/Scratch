#### Hyperparameter Optimization
Gaussian Process Regression (GPR) is among the best predictors avaialable, and has a particular advantage for hyperparameter optimization:  the hyperparameters become parameters of the model itself, and are estimated at the same time as the model.

mlrMBO uses Gaussian Process Regression with a Radial Basis Function (also known as Squared Euclidean) as the kernel when all variables are continuous.  When categorical factors are added, it proposes instead to use Random Forest Regression (or any other predictor with a standard error) as a surrogate.  This approach is problematic, as has been noted in [Remarks](https://mlr-org.github.io/mlrMBO/articles/supplementary/mixed_space_optimization.html#remarks) by the authors of mlrMBO. They comment that

>For the random forest the uncertainty is based on the diversity of the results of the individual individual forests.

However, this could be amplified to note that the construction of the forest itself will seek to minimize this diversity.  In practice, the Random Forest in mlrMBO will tend to identify and explore near the best value observed so far.  (This behavior has been explored in some detail with the aid of a Python/scikit-learn mockup.)

However, one major advantage of Gaussian Process Regression is that hyperparameters are included in fitting the model itself.  This motivates finding an alternative that can accomodate the categorical variables as part of the model.  A simple approach is simply to add dummy-coded indicator variables to an RBF kernel, sometimes described as Automatic Relevance Detection, or ARD.  If the associated length-scale parameter is large, the parameter is essentially ignored in the model.
This approach was tested with success for the NT3 model, as shown in [mlskMBO (replacing R with scikit-learn in MBO](../tree/master/mlskMBO).  After training on a small sample from a grid, it was able to make predictions which substantially outperformed the training data. 

A much richer model for categorical factors has been proposed by [Zhou, Quan and Zhou](http://amstat.tandfonline.com/doi/full/10.1198/TECH.2011.10025). This model uses a parameterization of the hypersphere to model correlations between different levels of a factor, and has been implemented in Python.  Three correlation kernels are implemented: Exhangeable, Multiplicative, and Unrestrictive.  (The first two correspond to using dummy codes with a Squared Euclidean (RBF) kernel, with a single length-scale or individual parameters, but with an explicit representation as a correlation matrix.) These are fully integrated into the scikit-learn Gaussian Process Regression module.  The most time-consuming portions of the calculation have been isolated and implemented as compiled C functions using Cython, [hypersphere](../hypershpere) allowing parallel computation using OpenMP when available.  The kernel used is a product of an RBF on the continuous variables, a scale factor, and a correlation kernel for each categorical factor.  (A small noise term is added to the RBF kernel as well.)

Testing P3B1 with a GPR Qualitative Kernel model, using each point in the training data as a starting point, there emerge at least five local minima with a large number of training parameters belonging to each.  Since the P3B1 problem was created by averaging together the losses from five subproblems, the working hypothesis is that each subproblem has a different optimal set of paramaters, but there is no 'compromiae' set of parameters which works well on average.

Testing P1B1 is problematic for a number of reasons.  The 'model' parameter takes on values of 'ae' or 'vae'.  Trained on a subset of 1680 runs on Titan, The model shows these as having a correlation of -1, while all the other categorical factors have correlations of 1, i.e. they are irrelevant.  In fact, upon inspection all validation losses from 'ae' are around 0.25 or below, while all losses for 'vae' are above 0.66.  Therefore, consider restricting the GPR model to one 'model' value at a time.  Since about half the parameters for 'vae' produced NaN or failed to run in keras, restrict to 'ae'.

##### Future Directions
An unavoidable difficulty for any kernel learning method is that as the number of training examples N grows, the size of the kernel matrix is N^2 and running times for associated algorithms typically grows as O(N^3).  One solution to this, currently regarded as state-of-the-art, is the Sparse Pseudo-Input Gaussian Process (SPGP) of [Snelson and Ghahramani](http://www.gatsby.ucl.ac.uk/~snelson/SPGP_up.pdf).  This approximates the N*N kernel matrix with a smaller M*M matrix, and has running times which are O(N*M^2).  

After implementing the qualitative kernels of Zhou et al for scikit-learn, it would be straightforward to set up the kernel used for SPGP.  (This would also allow a simpler low-rank representation, which would model correlation between variables.)

Moreover, it has been noted [Titsias] that the SPGP can be too flexible and prone to overfitting. The hypersphere coordinates ddeveloped for the Unrestrictive Correlation model could be used to directlyparameterize the Cholesky decomposition of the M*M matrix used in computations, avoiding the computational overhead of solving for it.  Together with a parameterization of the null space of the low-rank decomposition, this would both streamline computation and reduce the over-parameterization and overfitting problem sometimes observed in SPGP.
